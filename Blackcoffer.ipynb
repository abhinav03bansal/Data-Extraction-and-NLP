{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1rPiwzvYPkZ-MG_rYjvJQGNfDA3hBVJI3","authorship_tag":"ABX9TyPRwGejIcDL9ctbTazzg3uU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#mounting drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ps2lTwpksX8g","executionInfo":{"status":"ok","timestamp":1709569014557,"user_tz":-330,"elapsed":16476,"user":{"displayName":"Abhinav Bansal","userId":"12735139842800038214"}},"outputId":"520d630b-2652-4206-a53d-98654f20d6ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["#Parsing and extracting paragraphs from all site\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","\n","\n","def extract_paragraphs(url, container_class):\n","    # Send a GET request to the URL\n","    response = requests.get(url)\n","    htmlcontent=response.content\n","    # Check if the request was successful (status code 200)\n","    if response.status_code == 200:\n","        # Parse the HTML content of the page\n","        soup = BeautifulSoup(response.content, 'html.parser')\n","        title=soup.title.string\n","        # Find the container div with the specified class\n","        container_div = soup.find('div', class_=container_class)\n","\n","        if container_div:\n","            # Extract paragraphs from the container div\n","            paragraphs = container_div.find_all('p')\n","\n","            # Extract the text content of each paragraph\n","            paragraph_contents = [p.get_text() for p in paragraphs]\n","\n","            return paragraph_contents\n","        else:\n","            print(f\"Container div with class '{container_class}' not found.\")\n","            return None\n","    else:\n","        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n","        return None\n","\n","\n","def save_to_file(url_id, paragraphs):\n","    # Create a filename using the URL_ID\n","    filename = f\"{url_id}.txt\"\n","    full_path = f\"{'/content/drive/MyDrive/ColabNotebooks/'}{filename}\"\n","    # Write paragraphs to the file\n","    with open(full_path, 'w', encoding='utf-8') as file:\n","        for i, paragraph in enumerate(paragraphs, start=1):\n","            file.write(f\"Paragraph {i}:\\n{paragraph}\\n\\n\")\n","\n","    print(f\"Text saved to file: {filename}\")\n","\n","def driver_code():\n","  ip=pd.read_excel(\"/content/drive/MyDrive/ColabNotebooks/Input.xlsx\")\n","  container_class = \"td-post-content tagdiv-type\"\n","  for i in range(1,100):\n","    url_id=ip.iloc[i][0]\n","    url=ip.iloc[i][1]\n","    paragraphs = extract_paragraphs(url,container_class)\n","    save_to_file(url_id,paragraphs)\n","\n","\n","driver_code()\n","\n","\n","\n","\n"],"metadata":{"id":"hcSRnxJFaiWY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Concactenating all files into one for analysing\n","import os\n","import pandas as pd\n","file_path=\"/content/drive/MyDrive/abc\"\n","file_list=os.listdir(file_path)\n","file_path_list=[f'/content/drive/MyDrive/abc/{i}' for i in file_list]\n","\n","def concat():\n","  with open('/content/drive/MyDrive/ColabNotebooks/out.txt','w',encoding='utf-8') as outfile:\n","    for i in file_path_list:\n","      with open(i) as infile:\n","        for line in infile:\n","          outfile.write(line)\n","\n","concat()"],"metadata":{"id":"6GvMvuTLY5ow"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# removing stop words\n","def remove_words(input_file, words_file, output_file):\n","    # Read words to be removed from the second file\n","    with open(words_file, 'r', encoding='utf-8') as words_file:\n","        words_to_remove = set(word.strip() for word in words_file)\n","\n","    # Read content from the first file and remove specified words\n","    with open(input_file, 'r', encoding='utf-8') as input_file:\n","        content = input_file.read()\n","        content_words = content.split()\n","\n","        filtered_words = [word for word in content_words if word not in words_to_remove]\n","        modified_content = ' '.join(filtered_words)\n","\n","    # Write the modified content to a new file\n","    with open(output_file, 'w', encoding='utf-8') as output_file:\n","        output_file.write(modified_content)\n","\n","if __name__ == \"__main__\":\n","    input_file_path = '/content/drive/MyDrive/ColabNotebooks/out.txt'\n","    words_file_path = '/content/drive/MyDrive/STPW/word.txt'\n","    output_file_path = '/content/drive/MyDrive/after.txt'\n","\n","    remove_words(input_file_path, words_file_path, output_file_path)\n","    print(\"Words removed successfully. Check the '{}' file.\".format(output_file_path))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W_6FgHpswAmN","executionInfo":{"status":"ok","timestamp":1709612016081,"user_tz":-330,"elapsed":1249,"user":{"displayName":"Abhinav Bansal","userId":"12735139842800038214"}},"outputId":"c0dfa1e1-b89d-4f39-92d4-3995e89a01c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Words removed successfully. Check the '/content/drive/MyDrive/after.txt' file.\n"]}]},{"cell_type":"code","source":["# tk\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","\n","\n","file_path = '/content/drive/MyDrive/after.txt'\n","with open(file_path, 'r', encoding='utf-8') as file:\n","    text = file.read()\n","tokens = word_tokenize(text)\n","with open('/content/drive/MyDrive/ColabNotebooks/positive-words.txt', 'r', encoding='utf-8') as file:\n","    text = file.read()\n","pos = word_tokenize(text)\n","\n","pos_count=0\n","for i in pos:\n","  if i in tokens:\n","    pos_count+=1\n","\n","print(pos_count)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DUtZIbdt3256","executionInfo":{"status":"ok","timestamp":1709613781343,"user_tz":-330,"elapsed":3414,"user":{"displayName":"Abhinav Bansal","userId":"12735139842800038214"}},"outputId":"8017dc2b-f02c-4231-916e-b5b4e53d4173"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["545\n"]}]},{"cell_type":"code","source":["# tk\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","\n","\n","file_path = '/content/drive/MyDrive/after.txt'\n","with open(file_path, 'r', encoding='utf-8') as file:\n","    text = file.read()\n","tokens = word_tokenize(text)\n","with open('/content/drive/MyDrive/neg-words.txt', 'r', encoding='utf-8') as file:\n","    text = file.read()\n","neg = word_tokenize(text)\n","\n","neg_count=0\n","for i in neg:\n","  if i in tokens:\n","    neg_count+=1\n","\n","print(neg_count)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AVgbTPWh7nLz","executionInfo":{"status":"ok","timestamp":1709613992546,"user_tz":-330,"elapsed":7267,"user":{"displayName":"Abhinav Bansal","userId":"12735139842800038214"}},"outputId":"770f52a5-1569-4084-aab0-e52773121906"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["640\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from string import punctuation\n","\n","# Download the necessary resource (you only need to do this once)\n","nltk.download('punkt')\n","\n","# Read text from a file\n","file_path = '/content/drive/MyDrive/after.txt'\n","with open(file_path, 'r', encoding='utf-8') as file:\n","    text = file.read()\n","\n","# Tokenize the text using NLTK and exclude punctuation\n","tokens = word_tokenize(text)\n","filtered_tokens = [word.lower() for word in tokens if word.isalnum()]\n","\n","# Count the total number of words\n","word_count = len(filtered_tokens)\n","\n","# Print the result\n","print(f\"Total number of words (excluding punctuation) in the file: {word_count}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WZzVFHWr8fby","executionInfo":{"status":"ok","timestamp":1709614320430,"user_tz":-330,"elapsed":718,"user":{"displayName":"Abhinav Bansal","userId":"12735139842800038214"}},"outputId":"007ca613-a98d-4c99-fd10-a2ed79e4d933"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Total number of words (excluding punctuation) in the file: 53705\n"]}]},{"cell_type":"code","source":["Polarity_Score = ((pos_count-neg_count)/ ((pos_count + neg_count) + 0.000001))\n","print(Polarity_Score)\n","\n","Subjectivity_Score = ((pos_count + neg_count)/ ((word_count) + 0.000001))\n","print(Subjectivity_Score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jqr2k7qx78G7","executionInfo":{"status":"ok","timestamp":1709614366754,"user_tz":-330,"elapsed":430,"user":{"displayName":"Abhinav Bansal","userId":"12735139842800038214"}},"outputId":"a1a61538-a4f2-4db3-8f41-fe90fb42e1e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-0.08016877630365503\n","0.02206498463789098\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import sent_tokenize\n","\n","# Download the necessary resource (you only need to do this once)\n","nltk.download('punkt')\n","\n","# Read text from a file\n","file_path = '/content/drive/MyDrive/after.txt'  # Replace with the path to your text file\n","with open(file_path, 'r', encoding='utf-8') as file:\n","    text = file.read()\n","\n","# Tokenize the text into sentences\n","sentences = sent_tokenize(text)\n","\n","# Count the number of sentences\n","sentence_count = len(sentences)\n","\n","asl=word_count/sentence_count\n","print(asl)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RWB3Z95F9TBa","executionInfo":{"status":"ok","timestamp":1709614498120,"user_tz":-330,"elapsed":433,"user":{"displayName":"Abhinav Bansal","userId":"12735139842800038214"}},"outputId":"a8a14dff-f83a-43fe-81f3-cd67e55aec34"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of sentences in the file: 4667\n","11.507392329119348\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.corpus import words\n","import string\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('words')\n","def preprocess_text(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        text = file.read()\n","\n","    # Tokenize the text\n","    words = word_tokenize(text)\n","\n","    # Remove punctuation marks\n","    words = [word for word in words if word.isalnum()]\n","\n","    # Remove articles (common English stopwords)\n","    stop_words = set(stopwords.words('english'))\n","    words = [word for word in words if word.lower() not in stop_words]\n","\n","    return words\n","\n","# Example usage:\n","file_path = '/content/drive/MyDrive/after.txt'\n","tokenized_text = preprocess_text(file_path)\n","\n","\n","\n","def count_complex_words(word_list):\n","    english_word_set = set(words.words())\n","    complex_word_count = sum(1 for word in word_list if word not in english_word_set)\n","    return complex_word_count\n","\n","result = count_complex_words(tokenized_text)\n","\n","pcw=result /word_count\n","print(pcw)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1xH3k86f-AhW","executionInfo":{"status":"ok","timestamp":1709620099681,"user_tz":-330,"elapsed":1241,"user":{"displayName":"Abhinav Bansal","userId":"12735139842800038214"}},"outputId":"c9ad0da8-c0b4-4bde-b44e-542b8805b300"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Number of complex words: 20971\n","0.39048505725723864\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","\n","def average_sentence_length(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        text = file.read()\n","\n","    sentences = nltk.sent_tokenize(text)\n","    total_words = sum(len(nltk.word_tokenize(sentence)) for sentence in sentences)\n","    total_sentences = len(sentences)\n","\n","    if total_sentences == 0:\n","        return 0  # Avoid division by zero\n","\n","    avg_sentence_length = total_words / total_sentences\n","    return avg_sentence_length\n","\n","file_path = '/content/drive/MyDrive/ColabNotebooks/out.txt'\n","result = average_sentence_length(file_path)\n","\n","print(f\"Average sentence length: {result:.2f} words\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ASz_-w3DTdph","executionInfo":{"status":"ok","timestamp":1709620227213,"user_tz":-330,"elapsed":1206,"user":{"displayName":"Abhinav Bansal","userId":"12735139842800038214"}},"outputId":"e5762cef-f3c3-435d-ef65-c39f9b3cf976"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Average sentence length: 24.19 words\n"]}]},{"cell_type":"code","source":["fog_index= 0.4 * (result + pcw)\n","print(fog_index)\n","anwps=word_count/sentence_count\n","print(anwps)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rb65HNFuTqfX","executionInfo":{"status":"ok","timestamp":1709620521938,"user_tz":-330,"elapsed":426,"user":{"displayName":"Abhinav Bansal","userId":"12735139842800038214"}},"outputId":"df01a6fe-99dc-4eee-92a1-938bf1fc5655"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["1866.956194022903\n","11.507392329119348\n"]}]},{"cell_type":"code","source":["import string\n","\n","def count_words_without_punctuation(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        text = file.read()\n","\n","    # Remove punctuation and split the text into words\n","    words = ''.join(char if char not in string.punctuation else ' ' for char in text).split()\n","\n","    # Count the number of words\n","    num_words = len(words)\n","\n","    return num_words\n","\n","# Example usage:\n","file_path = '/content/drive/MyDrive/after.txt'\n","result = count_words_without_punctuation(file_path)\n","\n","print(f\"Number of words (without punctuation): {result}\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IVeplLYbUF9Q","executionInfo":{"status":"ok","timestamp":1709620609019,"user_tz":-330,"elapsed":420,"user":{"displayName":"Abhinav Bansal","userId":"12735139842800038214"}},"outputId":"3c9cbb34-4493-4d71-c1c5-ce6feb02d773"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of words (without punctuation): 55810\n"]}]},{"cell_type":"code","source":["import re\n","\n","def count_personal_pronouns_in_file(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        text = file.read()\n","\n","    # Define the regex pattern for personal pronouns excluding \"US\" as a country\n","    pattern = r'\\b(?:I|we|my|ours|us)\\b'\n","\n","    # Use re.findall to find all matches in the text\n","    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n","\n","    # Count the number of matches\n","    count = len(matches)\n","\n","    return count\n","\n","# Example usage:\n","file_path = '/content/drive/MyDrive/ColabNotebooks/out.txt'\n","result = count_personal_pronouns_in_file(file_path)\n","\n","print(f\"Number of personal pronouns: {result}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3yZEZ6kYVYmB","executionInfo":{"status":"ok","timestamp":1709620714592,"user_tz":-330,"elapsed":409,"user":{"displayName":"Abhinav Bansal","userId":"12735139842800038214"}},"outputId":"a5d99663-e7dc-4c7a-a68c-b583d82baac0"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of personal pronouns: 607\n"]}]},{"cell_type":"code","source":["import re\n","\n","def count_syllables(word):\n","    # Define exceptions for syllable counting\n","    exceptions = [\"es\", \"ed\"]\n","\n","    # Remove trailing \"es\" or \"ed\" before counting syllables\n","    for exception in exceptions:\n","        if word.endswith(exception):\n","            word = word[:-len(exception)]\n","\n","    # Use regex to count syllables based on vowels\n","    syllables = re.findall(r'[aeiouy]+', word, flags=re.IGNORECASE)\n","\n","    # Handle special cases where 'e' at the end is not counted as a syllable\n","    if word.endswith('e') and len(syllables) > 1:\n","        syllables.pop()\n","\n","    return len(syllables)\n","\n","def count_syllables_in_file(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        text = file.read()\n","\n","    # Tokenize the text into words\n","    words = re.findall(r'\\b\\w+\\b', text)\n","\n","    # Count syllables for each word\n","    syllable_counts = [count_syllables(word) for word in words]\n","\n","    return syllable_counts\n","\n","\n","file_path = '/content/drive/MyDrive/ColabNotebooks/out.txt'\n","syllable_counts = count_syllables_in_file(file_path)\n","res=sum(syllable_counts)\n","print(f\"Syllable counts for each word: {res}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VU8RmXBuVquc","executionInfo":{"status":"ok","timestamp":1709620922141,"user_tz":-330,"elapsed":833,"user":{"displayName":"Abhinav Bansal","userId":"12735139842800038214"}},"outputId":"b86d7941-4ce6-47eb-bc25-156bd1d0dd02"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Syllable counts for each word: 162145\n"]}]},{"cell_type":"code","source":["def calculate_word_lengths(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        text = file.read()\n","\n","    # Tokenize the text into words\n","    words = text.split()\n","\n","    # Calculate the length of each word\n","    word_lengths = [len(word) for word in words]\n","\n","    return word_lengths\n","\n","# Example usage:\n","file_path = '/content/drive/MyDrive/ColabNotebooks/out.txt'\n","word_lengths = calculate_word_lengths(file_path)\n","res=sum(word_lengths)\n","print(f\"Total number of characters for each word: {res}\")\n","\n","avg_word_len=res/word_count\n","print(avg_word_len)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fw2rQsNOWepv","executionInfo":{"status":"ok","timestamp":1709621073999,"user_tz":-330,"elapsed":607,"user":{"displayName":"Abhinav Bansal","userId":"12735139842800038214"}},"outputId":"caad98dd-7c85-4720-b168-a310c69a3391"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of characters for each word: 521974\n","9.719281258728238\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"U4cjWntBYTQl"}}]}